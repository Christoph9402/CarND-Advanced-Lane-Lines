# CarND-Advanced-Lane-Lines

The goals / steps of this project are the following:  
•	Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.  
•	Apply a distortion correction to raw images.  
•	Use color transforms, gradients, etc., to create a thresholded binary image.  
•	Apply a perspective transform to rectify binary image ("birds-eye view").  
•	Detect lane pixels and fit to find the lane boundary.  
•	Determine the curvature of the lane and vehicle position with respect to center.  
•	Warp the detected lane boundaries back onto the original image.  
•	Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.  

# Description of Pipeline
The first step for finding lane lines is calibrating the camera. For that 2 variables nx and ny are defined. They represent the number of corners of the chessboard that is used for calibration, in this case 9x6. Afterwards a path for the image is defined and the image is loaded. A function called “camera_cal” is used to calibrate the camera.  Inside this function, the glob function is first used to address all images, which names start with the word “calibration”. Then, two empty lists – objpts and imgpts - are created. Another array calles objp was created and filled with zeros. Then it was filled with values for each corner ( (0,0,0) to (9,6,0)). Afterwards, the program loops through each image inside the collection of images. Inside this loop. Each image of the chessboard is first read and converted to a grayscale image using the cv2.ctvColor() function. The cv2.findChessboardCorners function is then applied to the grayscale image. This function returns ret and the corners of the image. If the ret value is false, which means that no corners were found, the next chessboard image is used. If on the other hand ret is true, the corners are appended to the imgpts function and objp are appended to the objpts list. Using the drawChessboradCorners function, those corners are displayed on the image as seen in image 1b. Afterwards the cv2.calibrateCamera() function is applied to get the camera matrix as well as dist as a return. Those are necessary for the distortion correction. The next step in the pipeline is calling the function dist_correction(), which returns a undistorted image. This function uses the cv2.undistort() function and the camera matrix, an image and dist as input. The returned undistorted image is the basis for analysing the image for lane lines. Examples can be seen in image 1c for the chessboard and a comparison for an image taken on the road can be seen in image 2a and 2b. 
In the next step, the undistorted image is being processed so that pixel of lane lines can be detected. For this, the image is first converted into HLS color space using the cv2.cvtColor() function, where the undistorted image is used as an input. Afterwards the saturation channel is extracted using S=HLS[ :, :, 2]. The S-channel of the frame can be seen in image 3a. Then a threshold is being set. After testing, I decided to use 170 as a minimum and 255 as a maximum threshold. Then, the function col_bin is being called. Inside this function, another function is called – the abs_sobel() function. Here, the image is first converted into a grayscale. Then the cv2.sobel() function is applied in x orientation and with a threshold of (0,255) and a kernel size of 7. The absolute sobel and the scaled sobel are calculated before creating a binary image, which is the return of the abs_sobel_thresh function. Inside de col_sobel function, another binary image is being created, in this however, it is not the gradient that is considered, but the saturation of the image. The col_bin function returns the binary image of the Saturation channel (cbinary) and the binary image from the abs_sobel_thresh function (gradient). In the pipeline, those two binary images are then combined into a single binary using the comb_binary() function. The binary images can be seen in images 3b and 3c.
The next main step in the pipeline is, to transform the binary image to a birds eye view, which is later used to find the lane pixels. For this, source and destination points are defined. I decided to use [195,720], [555,470], [730,470], [1120,720] as source points and [195,720], [195,0], [1120,0], [1120,720] as destination points. Then the apply_persp_transform() function is applied, which uses the combined binary image, the destination and the source points. Inside this function, a matrix M is first being calculated using the function cv2.getPerspectiveTransform() and the source and destination points. Then, the function cv2.warpPerspective() is used to transform the perspective of an image to a birds-eye view, which is the return of the apply_persp_transform() function. An example of the transformed perspective can be seen in image 4a.
Afterwards, the function for detecting the lane pixels is applied.  For this, two different functions were created. A function sliding_window() is applied to images and in case of videos to every first and twenty-fifth frame. For the remaining frames of a video, another function search_around() is applied. First, the sliding_window() function is being described:
Inside the sliding_window() function, the warped image is analysed by first creating a histogram of the lower half of the transformed binary image. Then, the middle of the image is calculated, and an output image is being created.  The base x values for the left and right lane are calculated by searching for the maximum of each half of the histogram. Then, a few parameters are created for finding lane lines – the number of windows n that will be created, a margin and the minimum number of pixels. I decided to use n=5, a marging of 100 and a minpix of 40. Then two empty lists – left_lane_inds and right_lane_inds are created. For every window the lower and upper y-values are created as well as the min and max x values for the left and right lane using the margin, considering the current x position calculated earlier. After that, only the active pixels in these boundaries are filtered. There are appended to the lists left_lane_inds or right_lane_inds. Then, the current x position is adjusted, in case that the length of the list good_left_inds and good_right_inds is greater than the minimum number of pixels declared earlier. Afterwards, two second degree polynomials are created for the left and right lane using the polyfit() function. A vector ploty, which consists of a series of numbers from 0 to 720 is created. Those numbers are later used for the polinominals. Left_lane and right_lane consist of every value that represents the lanes. Two empty arrays, ptsl and ptsr are created. They are filled with points from left_lane and right_lane as well as ploty. For the right lane, the list is flipped. Those two arrays are needed to draw the area between the left and right lane onto the image. This area is created using the cv2.fillPoly() function. The pixels of the left lane are represented in red and the pixels of the right lane are blue.  The result of the sliding window function is created using the addweighted() function. The sliding_window function returns the resulted image, the ploty vector as well as the 2 polynomials of the left (left_fit) and right lane (right_fit). The detected lane can also be seen in image 4a.
Inside the main code the returned polynomials right_fit, left_fit as well as ploty are used to calculate the radius of the curve. This is done by calling a function radius(). Inside this function, two variables are first defined, which are used to convert pixel into meters. I decided to use a value of 3.7/900 for the x-direction, because looking at the warped image, it is visible, that the two lanes are approximately 900 pixels apart at the lower edge. Then a variable y_eval was created which represents the maximum value of the ploty vector. He radius of the left and right lane was then calculated using the formula R=((1+(2*Ay+B)^2)^(3/2))/(abs(2*A) for each lane. He radius of the middle of the lane was calculated using np.mean().Then a variable mid was initialized which represents the middle of the frame, which is the number of pixels divided by 2 (1280/2). The vehicles offset could be calculated taking the absolute of the difference of the mid value and the mean lane radius at the bottom of the frame. This value is then multiplied by xm_per pixel to convert the result to meters. The function returns the offset as well as the radius. Inside the main code, those values are rounded, converted to a string and using the cv2.puttext() function printed onto the upper left corner of the final image after transforming the perspective back. 
To transform the image back, a function rev_persp_transform() is called. It uses the image from the sliding_window() function as well as the source and destination points that have been used before. As before, a matrix M is first created using the cv2.getPerspectiveTransform() function. In this case however, source and destination points are switched. Afterwards, the unwarped image is created by using the warpPerspective function again. The result can be seen in image 4b. The final image is created by using the addWeighted() function(), where the undistorted image is combined with the image in which the lane is drawn on. It is shown in the images5a to 5h.
As mentioned earlier, for videos the sliding_windows() function I only used for the first and every 25th frame. The camera calibration is applied to every frame. Another peculiarity with videos is, that a smoothing function is implemented, which is called every frame before calculating the radius. Input parameters for the smoothing function are on the polynomials of the left and right lane. From those, the coefficients are extracted and appended to a list each. If however the length of the lists is greater than fifty, the first row of each list is deleted. That way, only the last 50 frames are used. To smooth the coefficients, an average is calculated for each of the six lists by dividing the sum of the elements of each list by the length of the list. Those values (avg_l0, avg_l1, avg_l2, avg_r0, avg_r1 and avg_r2) are then returned by the smooth() function. Inside the main function those average coefficients are then used to smooth the radius and offset of the car. 
To calculate the remaining frames for a video, a search_around() function is used. It uses the coefficients for the left and right lane provided by the smooth function. Then a margin is initialized with a value if 100. This margin represents the area around the polynomials to left and right, in which the algorithm should look for lane pixels. Analog to the Udacity exercise, right_lane_inds and left_lane_inds were calculated by looking, if there were nonzero pixels within the boundary set by the polynomials and the margin. Using polyfit() new polynomials were created. Just as in the sliding_window() function, the arrays ptsl and ptsr containing the points for drawing the area onto the image, were created. Using the cv2.fillPoly function, areas around the Polynomials where lane pixels are searched can be visualized. For this, points around the left and right lane were calculated. I decided to not display them to get a clearer image of the area between the lanes The function returns the resulted image, which was again calculated using the addWeighted() function and it also returns the new polynomials of the left and right lane. In the main code, those are then used by the smooth function.
Every time the sliding_windows() function or the search_around() function is called, a global variable “frameno” which was initialized before loading the image, is increased by one. This variable is used to keep track of the frame number so that the program can decide, which function to call.
